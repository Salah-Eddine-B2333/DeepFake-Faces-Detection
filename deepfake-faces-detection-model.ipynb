{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3134515,"sourceType":"datasetVersion","datasetId":1909705}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/bouazasalaheddine/deepfake-detection-with-transfer-learning?scriptVersionId=235068704\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Pre-processing dataset using OpenCV to detect faces","metadata":{}},{"cell_type":"code","source":"import os\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport cv2\nimport numpy as np\nfrom PIL import Image\n\n# Set target dimension to 256x256 for compatibility with common models\ndimension = (256, 256)\n\n# Define dataset paths\ndataset_path = '/kaggle/input/deepfake-and-real-images/Dataset'\noriginal_train_dir = os.path.join(dataset_path, 'Train')\noriginal_val_dir = os.path.join(dataset_path, 'Validation')\noriginal_test_dir = os.path.join(dataset_path, 'Test')\n\n# Load ESRGAN model from TensorFlow Hub\nesrgan_path = \"https://tfhub.dev/captain-pool/esrgan-tf2/1\"\nmodel = hub.load(esrgan_path)\n\ndef preprocessing(img):\n    \"\"\"Prepare images for ESRGAN model input\"\"\"\n    if img.shape[0] < 4 or img.shape[1] < 4:\n        raise ValueError(\"Image dimensions must be at least 4x4 pixels.\")\n    \n    # Calculate valid crop size divisible by 4\n    image_size = (tf.convert_to_tensor(img.shape[:-1]) // 4) * 4\n    cropped_image = tf.image.crop_to_bounding_box(\n        img, 0, 0, image_size[0], image_size[1])\n    \n    # Normalize to [0, 1] range expected by ESRGAN\n    preprocessed_image = tf.cast(cropped_image, tf.float32) / 255.0\n    return tf.expand_dims(preprocessed_image, 0)\n\ndef srmodel(img):\n    \"\"\"Enhance image using ESRGAN model\"\"\"\n    preprocessed_image = preprocessing(img)\n    new_image = model(preprocessed_image)\n    return tf.squeeze(new_image)\n\n# Set up preprocessing directories\npreprocessed_base = '/kaggle/working/preprocessed_dataset'\npreprocessed_train_dir = os.path.join(preprocessed_base, 'Train')\npreprocessed_val_dir = os.path.join(preprocessed_base, 'Validation')\npreprocessed_test_dir = os.path.join(preprocessed_base, 'Test')\n\n# Create directory structure\nos.makedirs(preprocessed_train_dir, exist_ok=True)\nos.makedirs(preprocessed_val_dir, exist_ok=True)\nos.makedirs(preprocessed_test_dir, exist_ok=True)\n\nfor class_name in ['Fake', 'Real']:\n    for dir_path in [preprocessed_train_dir, preprocessed_val_dir, preprocessed_test_dir]:\n        os.makedirs(os.path.join(dir_path, class_name), exist_ok=True)\n\n# Initialize face detector\nface_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n\ndef detect_and_save_faces(original_dir, preprocessed_dir):\n    \"\"\"Process images through face detection and enhancement pipeline\"\"\"\n    counter = 0\n    processed_count = 0\n    \n    for class_name in ['Fake', 'Real']:\n        class_path = os.path.join(original_dir, class_name)\n        preprocessed_class_path = os.path.join(preprocessed_dir, class_name)\n        \n        for image_name in os.listdir(class_path):\n            image_path = os.path.join(class_path, image_name)\n            img = cv2.imread(image_path)\n            \n            if img is None:\n                #print(f\"Warning: Could not read image {image_path}\")\n                continue\n                \n            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n            faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=4)\n            \n            if len(faces) == 0:\n                #print(f\"No faces detected in {image_path}\")\n                continue\n                \n            # Process largest face\n            x, y, w, h = sorted(faces, key=lambda f: f[2]*f[3], reverse=True)[0]\n            face = img[y:y+h, x:x+w]\n            face_rgb = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n            output_path = os.path.join(preprocessed_class_path, image_name)\n            #print(f\"faces detected in {image_path}\")\n            \n            try:\n                if w >= 256 and h >= 256:\n                    # Directly resize large enough faces\n                    resized_face = cv2.resize(face_rgb, dimension, interpolation=cv2.INTER_AREA)\n                else:\n                    # Enhance small faces with super-resolution\n                    face_tensor = tf.convert_to_tensor(face_rgb, dtype=tf.float32)\n                    sr_face = srmodel(face_tensor)\n                    sr_face = (sr_face.numpy() * 255).astype(np.uint8)\n                    resized_face = cv2.resize(sr_face, dimension, interpolation=cv2.INTER_CUBIC)\n                \n                # Save processed image\n                Image.fromarray(resized_face).save(output_path)\n                processed_count += 1\n                \n                # Progress reporting\n                if processed_count % 1000 == 0:\n                    counter += 1\n                    print(f\"Processed {counter * 1000} images\")\n                    \n            except Exception as e:\n                print(f\"Error processing {image_path}: {str(e)}\")\n\n# Execute preprocessing pipeline\nprint(\"Preprocessing training data...\")\ndetect_and_save_faces(original_train_dir, preprocessed_train_dir)\n\nprint(\"\\nPreprocessing validation data...\")\ndetect_and_save_faces(original_val_dir, preprocessed_val_dir)\n\nprint(\"\\nPreprocessing test data...\")\ndetect_and_save_faces(original_test_dir, preprocessed_test_dir)\n\nprint(\"\\nPreprocessing complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T13:05:46.364588Z","iopub.execute_input":"2025-03-22T13:05:46.364801Z","iopub.status.idle":"2025-03-22T16:19:40.855826Z","shell.execute_reply.started":"2025-03-22T13:05:46.364782Z","shell.execute_reply":"2025-03-22T16:19:40.854881Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Transfer learning using ResNet50","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np  \nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Dense, Dropout, Flatten, GlobalAveragePooling2D \nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau , ModelCheckpoint\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.metrics import AUC\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\n\n# Enable TPU strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.TPUStrategy(tpu)\n    print(\"Running on TPU\")\nexcept ValueError:\n    strategy = tf.distribute.get_strategy()\n    print(\"Running on CPU/GPU\")\n\n# Set the seed for reproducibility\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n# Define dataset paths\ndataset_path = '/kaggle/working/preprocessed_dataset'\ntrain_dir = os.path.join(dataset_path, 'Train')\nval_dir = os.path.join(dataset_path, 'Validation')\ntest_dir = os.path.join(dataset_path, 'Test')\n\n# Hyperparameters\nimg_size = (255, 255)\nbatch_size = 64\nepochs = 10  # Increased to allow early stopping to work effectively\nlearning_rate = 0.0001\n\n# Data generators\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=15,\n    zoom_range=0.15,\n    horizontal_flip=True,\n    width_shift_range=0.1,\n    height_shift_range=0.1\n)\nval_test_datagen = ImageDataGenerator(rescale=1./255)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    #target_size=img_size,\n    batch_size=batch_size,\n    class_mode='categorical'\n)\n\nval_generator = val_test_datagen.flow_from_directory(\n    val_dir,\n    #target_size=img_size,\n    batch_size=batch_size,\n    class_mode='categorical'\n)\n\ntest_generator = val_test_datagen.flow_from_directory(\n    test_dir,\n    #target_size=img_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    shuffle=False\n)\n\n# Model architecture\nstrategy = tf.distribute.MirroredStrategy()\nwith strategy.scope():\n    # Load Model without the top layer\n    base_model = ResNet50(weights=\"imagenet\", include_top=False, input_shape=(256, 256, 3))\n    # Add custom layers on top of the base model\n    x = GlobalAveragePooling2D()(base_model.output)\n    x = Dropout(0,3)(x)\n    x = Dense(256, activation='relu')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = Dense(2, activation='softmax')(x)\n\n    # Create the full model\n    model = Model(inputs=base_model.input, outputs=x)\n\n    # Compile the model\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss=\"binary_crossentropy\", metrics=[\"accuracy\", AUC(name='auc')])\n\n# Enhanced callbacks\nearly_stopping = EarlyStopping(\n    monitor='val_accuracy',\n    min_delta=0.5,  # Minimum accuracy improvement threshold (0.5%)\n    patience=8,       # Wait for 7 epochs before stopping\n    verbose=1,\n    mode='max',\n    restore_best_weights=False\n)\n#after the 5th ephoch we use this to reduce learning rate to get more stable results\nreduce_lr = ReduceLROnPlateau(\n    monitor='val_loss',\n    factor=0.5,\n    patience=5,\n    min_lr=1e-7,\n    verbose=1\n)\ncheckpoint_path = '/kaggle/working/140K_resnet50_model.keras'\n\n#we use model checkpoint to save best weights and we make that when the validation_accuracy increase\nmodel_checkpoint = ModelCheckpoint(\n    checkpoint_path,\n    monitor=\"val_accuracy\",\n    save_best_only=True,\n    mode=\"max\",\n    verbose=1\n)\n\n\n# Training with enhanced callbacks\nhistory = model.fit(\n    train_generator,\n    epochs=epochs,\n    validation_data=val_generator,\n    \n    callbacks=[early_stopping, reduce_lr , model_checkpoint],\n    verbose=1\n)\n\n# Evaluation\ntest_loss, test_accuracy, test_auc = model.evaluate(test_generator, verbose=0)\nprint(f'Test accuracy: {test_auc:.4f}')\n\n# Save model\nmodel.save(\"deepfake_model_ResNet50.h5\")\nmodel.save(\"deepfake_model_ResNet50.keras\")\n\n# Visualization\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'], label='Train Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n# Performance metrics\ny_true = test_generator.classes\ny_pred = np.argmax(model.predict(test_generator), axis=1)\nprint('Classification Report:')\nprint(classification_report(y_true, y_pred))\n \nprint('Confusion Matrix:')\nprint(confusion_matrix(y_true, y_pred))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T17:57:16.383169Z","iopub.execute_input":"2025-04-11T17:57:16.383464Z","iopub.status.idle":"2025-04-11T22:35:42.673625Z","shell.execute_reply.started":"2025-04-11T17:57:16.383442Z","shell.execute_reply":"2025-04-11T22:35:42.672692Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Transfer learning using MobileNetV2","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np  \nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import  Dense, Dropout , GlobalAveragePooling2D \nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau , ModelCheckpoint\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.metrics import AUC\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\n\n# Enable TPU strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.TPUStrategy(tpu)\n    print(\"Running on TPU\")\nexcept ValueError:\n    strategy = tf.distribute.get_strategy()\n    print(\"Running on CPU/GPU\")\n\n# Set the seed for reproducibility\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n# Define dataset paths\ndataset_path = '/kaggle/working/preprocessed_dataset'\ntrain_dir = os.path.join(dataset_path, 'Train')\nval_dir = os.path.join(dataset_path, 'Validation')\ntest_dir = os.path.join(dataset_path, 'Test')\n\n# Hyperparameters\nimg_size = (255, 255)\nbatch_size = 64\nepochs = 13  # Increased to allow early stopping to work effectively\nlearning_rate = 0.0001\n\n# Data generators\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=15,\n    zoom_range=0.15,\n    horizontal_flip=True,\n    width_shift_range=0.1,\n    height_shift_range=0.1\n)\nval_test_datagen = ImageDataGenerator(rescale=1./255)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    #target_size=img_size,\n    batch_size=batch_size,\n    class_mode='categorical'\n)\n\nval_generator = val_test_datagen.flow_from_directory(\n    val_dir,\n    #target_size=img_size,\n    batch_size=batch_size,\n    class_mode='categorical'\n)\n\ntest_generator = val_test_datagen.flow_from_directory(\n    test_dir,\n    #target_size=img_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    shuffle=False\n)\n\n# Model architecture\nstrategy = tf.distribute.MirroredStrategy()\nwith strategy.scope():\n    # Load Model without the top layer\n    base_model = MobileNetV2(weights=\"imagenet\", include_top=False, input_shape=(256, 256, 3))\n    # Add custom layers on top of the base model\n    x = GlobalAveragePooling2D()(base_model.output)\n    x = Dropout(0,3)(x)\n    x = Dense(256, activation='relu')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = Dense(2, activation='softmax')(x)\n\n    # Create the full model\n    model = Model(inputs=base_model.input, outputs=x)\n\n    # Compile the model\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss=\"binary_crossentropy\", metrics=[\"accuracy\", AUC(name='auc')])\n\n# Enhanced callbacks\nearly_stopping = EarlyStopping(\n    monitor='val_accuracy',\n    min_delta=0.5,  # Minimum accuracy improvement threshold (0.5%)\n    patience=8,       # Wait for 7 epochs before stopping\n    verbose=1,\n    mode='max',\n    restore_best_weights=False\n)\n#after the 5th ephoch we use this to reduce learning rate to get more stable results\nreduce_lr = ReduceLROnPlateau(\n    monitor='val_loss',\n    factor=0.5,\n    patience=5,\n    min_lr=1e-7,\n    verbose=1\n)\ncheckpoint_path = '/kaggle/working/MobileNetV2_model.keras'\n\n#we use model checkpoint to save best weights and we make that when the validation_accuracy increase\nmodel_checkpoint = ModelCheckpoint(\n    checkpoint_path,\n    monitor=\"val_accuracy\",\n    save_best_only=True,\n    mode=\"max\",\n    verbose=1\n)\n\n\n# Training with enhanced callbacks\nhistory = model.fit(\n    train_generator,\n    epochs=epochs,\n    validation_data=val_generator,\n    callbacks=[early_stopping, reduce_lr , model_checkpoint],\n    verbose=1\n)\n\n# Evaluation\ntest_loss, test_accuracy, test_auc = model.evaluate(test_generator, verbose=0)\nprint(f'Test accuracy: {test_auc:.4f}')\n\n# Save model\nmodel.save(\"deepfake_model_MobileNetV2.h5\")\nmodel.save(\"deepfake_model_MobileNetV2.keras\")\n\n# Visualization\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'], label='Train Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n# Performance metrics\ny_true = test_generator.classes\ny_pred = np.argmax(model.predict(test_generator), axis=1)\nprint('Classification Report:')\nprint(classification_report(y_true, y_pred))\n \nprint('Confusion Matrix:')\nprint(confusion_matrix(y_true, y_pred))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T13:57:58.243856Z","iopub.execute_input":"2025-04-12T13:57:58.244172Z","iopub.status.idle":"2025-04-12T19:03:33.084513Z","shell.execute_reply.started":"2025-04-12T13:57:58.244147Z","shell.execute_reply":"2025-04-12T19:03:33.082776Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CNN with RNN","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np  \nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Dense, Dropout, Flatten, GlobalAveragePooling2D ,SimpleRNN , Reshape\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\n\n# Enable TPU strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.TPUStrategy(tpu)\n    print(\"Running on TPU\")\nexcept ValueError:\n    strategy = tf.distribute.get_strategy()\n    print(\"Running on CPU/GPU\")\n\n# Set the seed for reproducibility\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n# Define dataset paths\ndataset_path = '/kaggle/working/preprocessed_dataset'\ntrain_dir = os.path.join(dataset_path, 'Train')\nval_dir = os.path.join(dataset_path, 'Test')\n\n# Hyperparameters\n#img_size = (128, 128)\nbatch_size = 512\nepochs = 30  # Increased to allow early stopping to work effectively\nlearning_rate = 0.00001\n\n# Data generators\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=15,\n    zoom_range=0.15,\n    horizontal_flip=True,\n    width_shift_range=0.1,\n    height_shift_range=0.1\n)\nval_test_datagen = ImageDataGenerator(rescale=1./255)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    #target_size=img_size,\n    batch_size=batch_size,\n    class_mode='categorical'\n)\n\nval_generator = val_test_datagen.flow_from_directory(\n    val_dir,\n    #target_size=img_size,\n    batch_size=batch_size,\n    class_mode='categorical'\n)\n\n# Model architecture\nwith strategy.scope():\n    model = Sequential([\n        # CNN part\n        Conv2D(32, (3, 3), activation='relu', input_shape=(256, 256, 3), padding='same'),\n        BatchNormalization(),\n        MaxPooling2D((2, 2)),\n        Conv2D(64, (3, 3), activation='relu', padding='same'),\n        BatchNormalization(),\n        MaxPooling2D((2, 2)),\n        Conv2D(128, (3, 3), activation='relu', padding='same'),\n        BatchNormalization(),\n        MaxPooling2D((2, 2)),\n        Conv2D(128, (3, 3), activation='relu', padding='same'),\n        BatchNormalization(),\n        MaxPooling2D((2, 2)),  # Output shape: (batch, 16, 16, 128)\n        \n        # Reshape for RNN (treat spatial dimensions as temporal)\n        Reshape((16*16, 128)),  # Now shape is (batch, timesteps=256, features=128)\n        \n        # RNN part\n        SimpleRNN(128, return_sequences=True, activation='relu'),\n        SimpleRNN(64, activation='relu'),\n        \n        # Output\n        Dense(2, activation='softmax')\n    ])\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n\n# Enhanced callbacks\nearly_stopping = EarlyStopping(\n    monitor='val_accuracy',\n    min_delta=0.5,  # Minimum accuracy improvement threshold (0.5%)\n    patience=14,       # Wait for 7 epochs before stopping\n    verbose=1,\n    mode='max',\n    restore_best_weights=True\n)\n\nreduce_lr = ReduceLROnPlateau(\n    monitor='val_loss',\n    factor=0.5,\n    patience=5,\n    min_lr=1e-6,\n    verbose=1\n)\n\n# Training with enhanced callbacks\nhistory = model.fit(\n    train_generator,\n    epochs=epochs,\n    validation_data=val_generator,\n    callbacks=[early_stopping, reduce_lr],\n    verbose=1\n)\n\n# Evaluate and save the model\ntest_loss, test_acc = model.evaluate(val_generator, verbose=0)\nprint(f'Test accuracy: {test_acc:.4f}')\n\nmodel.save(\"deepfake_model.h5\")\nmodel.save(\"deepfake_model.keras\")\n\n\n# Plot training history and generate reports\nplt.figure(figsize=(10, 6))\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'])\n\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'])\nplt.show()\n#test test \ny_true = val_generator.classes\ny_pred = np.argmax(model.predict(val_generator), axis=1)\nprint('Classification Report:') \nprint(classification_report(y_true, y_pred))\nprint('Confusion Matrix:')\nprint(confusion_matrix(y_true, y_pred))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T17:11:38.775044Z","iopub.execute_input":"2025-04-16T17:11:38.775451Z","iopub.status.idle":"2025-04-16T17:11:55.001581Z","shell.execute_reply.started":"2025-04-16T17:11:38.775419Z","shell.execute_reply":"2025-04-16T17:11:55.00038Z"}},"outputs":[{"name":"stdout","text":"Running on CPU/GPU\nFound 32454 images belonging to 1 classes.\nFound 9026 images belonging to 2 classes.\nEpoch 1/30\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-b272be6e5f70>\u001b[0m in \u001b[0;36m<cell line: 115>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;31m# Training with enhanced callbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m    116\u001b[0m     \u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/nn.py\u001b[0m in \u001b[0;36mcategorical_crossentropy\u001b[0;34m(target, output, from_logits, axis)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0me1\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0me2\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0me1\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0me2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    562\u001b[0m                 \u001b[0;34m\"Arguments `target` and `output` must have the same shape. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m                 \u001b[0;34m\"Received: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Arguments `target` and `output` must have the same shape. Received: target.shape=(None, 1), output.shape=(None, 2)"],"ename":"ValueError","evalue":"Arguments `target` and `output` must have the same shape. Received: target.shape=(None, 1), output.shape=(None, 2)","output_type":"error"}],"execution_count":2}]}